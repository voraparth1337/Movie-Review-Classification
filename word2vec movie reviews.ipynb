{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 25000 labeled train reviews, 25000 labeled test reviews, and 50000 unlabeled reviews\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.data\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "\n",
    "# loading labelled training data\n",
    "train = pd.read_csv( \"labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
    "\n",
    "# loading labelled testing data\n",
    "test = pd.read_csv( \"testData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
    "\n",
    "# loading unlabelled training data\n",
    "unlabeled_train = pd.read_csv( \"unlabeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
    "\n",
    "print \"Read %d labeled train reviews, %d labeled test reviews, and %d unlabeled reviews\\n\" % (train[\"review\"].size,  test[\"review\"].size, unlabeled_train[\"review\"].size )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_wordlist( review, remove_stopwords=False ):\n",
    "    words = review.lower().split()\n",
    "\n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "\n",
    "    # 5. Return a list of words\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function to split a review into parsed sentences\n",
    "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
    "    # Function to split a review into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    \n",
    "    review_text_1 = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review_text_1)\n",
    "\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append( review_to_wordlist( raw_sentence, remove_stopwords ))\n",
    "    \n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n",
      "Parsing sentences from unlabeled set\n"
     ]
    }
   ],
   "source": [
    "sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "print \"Parsing sentences from training set\"\n",
    "for review in train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)\n",
    "\n",
    "print \"Parsing sentences from unlabeled set\"\n",
    "for review in unlabeled_train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74995\n"
     ]
    }
   ],
   "source": [
    "print len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-27 14:36:27,846 : INFO : 'pattern' package not found; tag filters are not available for English\n",
      "2017-08-27 14:36:27,850 : INFO : collecting all words and their counts\n",
      "2017-08-27 14:36:27,851 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-27 14:36:28,757 : INFO : PROGRESS: at sentence #10000, processed 2385328 words, keeping 51525 word types\n",
      "2017-08-27 14:36:29,669 : INFO : PROGRESS: at sentence #20000, processed 4747257 words, keeping 67811 word types\n",
      "2017-08-27 14:36:30,534 : INFO : PROGRESS: at sentence #30000, processed 7099915 words, keeping 81670 word types\n",
      "2017-08-27 14:36:31,499 : INFO : PROGRESS: at sentence #40000, processed 9467635 words, keeping 93389 word types\n",
      "2017-08-27 14:36:32,313 : INFO : PROGRESS: at sentence #50000, processed 11865624 words, keeping 103474 word types\n",
      "2017-08-27 14:36:33,250 : INFO : PROGRESS: at sentence #60000, processed 14248671 words, keeping 112172 word types\n",
      "2017-08-27 14:36:33,884 : INFO : PROGRESS: at sentence #70000, processed 16609786 words, keeping 119828 word types\n",
      "2017-08-27 14:36:34,366 : INFO : collected 123500 word types from a corpus of 17796947 raw words and 74995 sentences\n",
      "2017-08-27 14:36:34,368 : INFO : Loading a fresh vocabulary\n",
      "2017-08-27 14:36:35,096 : INFO : min_count=40 retains 16489 unique words (13% of original 123500, drops 107011)\n",
      "2017-08-27 14:36:35,097 : INFO : min_count=40 leaves 17237790 word corpus (96% of original 17796947, drops 559157)\n",
      "2017-08-27 14:36:35,163 : INFO : deleting the raw counts dictionary of 123500 items\n",
      "2017-08-27 14:36:35,174 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2017-08-27 14:36:35,175 : INFO : downsampling leaves estimated 12748822 word corpus (74.0% of prior 17237790)\n",
      "2017-08-27 14:36:35,175 : INFO : estimated required memory for 16489 words and 300 dimensions: 47818100 bytes\n",
      "2017-08-27 14:36:35,266 : INFO : resetting layer weights\n",
      "2017-08-27 14:36:35,510 : INFO : training model with 4 workers on 16489 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-08-27 14:36:36,531 : INFO : PROGRESS: at 0.68% examples, 430858 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:36:37,533 : INFO : PROGRESS: at 1.42% examples, 456682 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:36:38,557 : INFO : PROGRESS: at 2.24% examples, 472179 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:36:39,558 : INFO : PROGRESS: at 2.92% examples, 463167 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:36:40,560 : INFO : PROGRESS: at 3.72% examples, 474362 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-27 14:36:41,564 : INFO : PROGRESS: at 4.45% examples, 471359 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:36:42,586 : INFO : PROGRESS: at 5.22% examples, 471880 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:36:43,593 : INFO : PROGRESS: at 6.01% examples, 474955 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:36:44,604 : INFO : PROGRESS: at 6.72% examples, 470075 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:36:45,609 : INFO : PROGRESS: at 7.32% examples, 461660 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:36:46,621 : INFO : PROGRESS: at 8.00% examples, 458203 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:36:47,640 : INFO : PROGRESS: at 8.77% examples, 459681 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:36:48,657 : INFO : PROGRESS: at 9.42% examples, 455744 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:36:49,663 : INFO : PROGRESS: at 10.31% examples, 463032 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:36:50,678 : INFO : PROGRESS: at 11.03% examples, 462670 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:36:51,683 : INFO : PROGRESS: at 11.87% examples, 467460 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:36:52,698 : INFO : PROGRESS: at 12.50% examples, 463610 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:36:53,707 : INFO : PROGRESS: at 13.35% examples, 467966 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:36:54,735 : INFO : PROGRESS: at 14.01% examples, 464935 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:36:55,740 : INFO : PROGRESS: at 14.88% examples, 469246 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:36:56,745 : INFO : PROGRESS: at 15.55% examples, 467608 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:36:57,754 : INFO : PROGRESS: at 16.15% examples, 463115 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:36:58,775 : INFO : PROGRESS: at 16.68% examples, 457661 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:36:59,775 : INFO : PROGRESS: at 17.58% examples, 461955 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-27 14:37:00,786 : INFO : PROGRESS: at 18.19% examples, 458782 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:37:01,791 : INFO : PROGRESS: at 18.97% examples, 460252 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-27 14:37:02,791 : INFO : PROGRESS: at 19.60% examples, 458304 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:37:03,794 : INFO : PROGRESS: at 20.44% examples, 460759 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:37:04,802 : INFO : PROGRESS: at 21.00% examples, 457438 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:37:05,808 : INFO : PROGRESS: at 21.80% examples, 458984 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:37:06,830 : INFO : PROGRESS: at 22.34% examples, 455271 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:37:07,831 : INFO : PROGRESS: at 23.18% examples, 457511 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:37:08,854 : INFO : PROGRESS: at 23.78% examples, 455255 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:37:09,863 : INFO : PROGRESS: at 24.66% examples, 458069 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:37:10,874 : INFO : PROGRESS: at 25.32% examples, 456516 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:37:11,876 : INFO : PROGRESS: at 26.18% examples, 459040 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:37:12,880 : INFO : PROGRESS: at 26.81% examples, 457065 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-27 14:37:13,886 : INFO : PROGRESS: at 27.35% examples, 454065 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:37:14,889 : INFO : PROGRESS: at 27.97% examples, 452529 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:37:15,905 : INFO : PROGRESS: at 28.60% examples, 450866 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:37:16,911 : INFO : PROGRESS: at 29.19% examples, 449104 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:37:17,913 : INFO : PROGRESS: at 29.84% examples, 448106 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:37:18,920 : INFO : PROGRESS: at 30.52% examples, 447897 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:37:19,935 : INFO : PROGRESS: at 31.12% examples, 446235 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-27 14:37:21,017 : INFO : PROGRESS: at 31.72% examples, 444416 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:37:21,993 : INFO : PROGRESS: at 32.28% examples, 442694 words/s, in_qsize 7, out_qsize 1\n",
      "2017-08-27 14:37:22,997 : INFO : PROGRESS: at 32.89% examples, 441599 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:37:23,998 : INFO : PROGRESS: at 33.74% examples, 443648 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:37:25,002 : INFO : PROGRESS: at 34.42% examples, 443454 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:37:26,005 : INFO : PROGRESS: at 35.27% examples, 445637 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:37:27,006 : INFO : PROGRESS: at 35.95% examples, 445328 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:37:28,023 : INFO : PROGRESS: at 36.49% examples, 443145 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:37:29,035 : INFO : PROGRESS: at 37.15% examples, 442647 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-27 14:37:30,039 : INFO : PROGRESS: at 38.05% examples, 444920 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:37:31,055 : INFO : PROGRESS: at 38.67% examples, 443865 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:37:32,068 : INFO : PROGRESS: at 39.49% examples, 445245 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:37:33,069 : INFO : PROGRESS: at 40.05% examples, 443624 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:37:34,099 : INFO : PROGRESS: at 40.84% examples, 444517 words/s, in_qsize 8, out_qsize 1\n",
      "2017-08-27 14:37:35,110 : INFO : PROGRESS: at 41.55% examples, 444644 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:37:36,122 : INFO : PROGRESS: at 42.40% examples, 446191 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:37:37,142 : INFO : PROGRESS: at 43.09% examples, 445867 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:37:38,144 : INFO : PROGRESS: at 43.89% examples, 447033 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:37:39,153 : INFO : PROGRESS: at 44.52% examples, 446159 words/s, in_qsize 8, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-27 14:37:40,169 : INFO : PROGRESS: at 45.39% examples, 447510 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:37:41,182 : INFO : PROGRESS: at 46.00% examples, 446618 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:37:42,193 : INFO : PROGRESS: at 46.89% examples, 448189 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:37:43,204 : INFO : PROGRESS: at 47.54% examples, 447612 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:37:44,226 : INFO : PROGRESS: at 48.43% examples, 449037 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:37:45,230 : INFO : PROGRESS: at 49.05% examples, 448239 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:37:46,241 : INFO : PROGRESS: at 49.90% examples, 449475 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:37:47,252 : INFO : PROGRESS: at 50.50% examples, 448528 words/s, in_qsize 7, out_qsize 2\n",
      "2017-08-27 14:37:48,254 : INFO : PROGRESS: at 51.41% examples, 450392 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:37:49,265 : INFO : PROGRESS: at 52.04% examples, 449763 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-27 14:37:50,276 : INFO : PROGRESS: at 52.95% examples, 451577 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:37:51,285 : INFO : PROGRESS: at 53.62% examples, 451041 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:37:52,286 : INFO : PROGRESS: at 54.43% examples, 452017 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-27 14:37:53,325 : INFO : PROGRESS: at 55.06% examples, 451312 words/s, in_qsize 8, out_qsize 1\n",
      "2017-08-27 14:37:54,332 : INFO : PROGRESS: at 55.91% examples, 452330 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:37:55,362 : INFO : PROGRESS: at 56.55% examples, 451604 words/s, in_qsize 6, out_qsize 1\n",
      "2017-08-27 14:37:56,358 : INFO : PROGRESS: at 57.45% examples, 452988 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:37:57,365 : INFO : PROGRESS: at 58.11% examples, 452568 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-27 14:37:58,374 : INFO : PROGRESS: at 58.91% examples, 453258 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:37:59,396 : INFO : PROGRESS: at 59.52% examples, 452420 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:38:00,401 : INFO : PROGRESS: at 60.43% examples, 453791 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:38:01,417 : INFO : PROGRESS: at 61.04% examples, 453110 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-27 14:38:02,446 : INFO : PROGRESS: at 61.89% examples, 453988 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:38:03,452 : INFO : PROGRESS: at 62.55% examples, 453589 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:38:04,458 : INFO : PROGRESS: at 63.36% examples, 454235 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:38:05,462 : INFO : PROGRESS: at 64.03% examples, 453928 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:38:06,477 : INFO : PROGRESS: at 64.82% examples, 454355 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:38:07,483 : INFO : PROGRESS: at 65.55% examples, 454435 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:38:08,517 : INFO : PROGRESS: at 66.38% examples, 454948 words/s, in_qsize 8, out_qsize 1\n",
      "2017-08-27 14:38:09,511 : INFO : PROGRESS: at 67.11% examples, 455036 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-27 14:38:10,515 : INFO : PROGRESS: at 67.89% examples, 455406 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:38:11,522 : INFO : PROGRESS: at 68.61% examples, 455311 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:38:12,531 : INFO : PROGRESS: at 69.38% examples, 455680 words/s, in_qsize 7, out_qsize 2\n",
      "2017-08-27 14:38:13,527 : INFO : PROGRESS: at 70.15% examples, 456037 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:38:14,555 : INFO : PROGRESS: at 70.87% examples, 455924 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:38:15,557 : INFO : PROGRESS: at 71.63% examples, 456285 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:38:16,563 : INFO : PROGRESS: at 72.36% examples, 456419 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:38:17,563 : INFO : PROGRESS: at 73.13% examples, 456831 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:38:18,575 : INFO : PROGRESS: at 73.91% examples, 457128 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:38:19,580 : INFO : PROGRESS: at 74.69% examples, 457577 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:38:20,606 : INFO : PROGRESS: at 75.44% examples, 457728 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:38:21,606 : INFO : PROGRESS: at 76.19% examples, 457847 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:38:22,607 : INFO : PROGRESS: at 76.86% examples, 457575 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:38:23,615 : INFO : PROGRESS: at 77.68% examples, 458062 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:38:24,640 : INFO : PROGRESS: at 78.37% examples, 457748 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:38:25,654 : INFO : PROGRESS: at 79.14% examples, 458033 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-27 14:38:26,661 : INFO : PROGRESS: at 79.85% examples, 457963 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:38:27,662 : INFO : PROGRESS: at 80.69% examples, 458668 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:38:28,667 : INFO : PROGRESS: at 81.35% examples, 458388 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:38:29,668 : INFO : PROGRESS: at 82.21% examples, 459182 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:38:30,701 : INFO : PROGRESS: at 82.94% examples, 459099 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:38:31,703 : INFO : PROGRESS: at 83.83% examples, 460040 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:38:32,711 : INFO : PROGRESS: at 84.53% examples, 459877 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:38:33,725 : INFO : PROGRESS: at 85.39% examples, 460452 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:38:34,725 : INFO : PROGRESS: at 86.09% examples, 460371 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:38:35,738 : INFO : PROGRESS: at 87.01% examples, 461290 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:38:36,744 : INFO : PROGRESS: at 87.64% examples, 460770 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:38:37,788 : INFO : PROGRESS: at 88.50% examples, 461263 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-27 14:38:38,819 : INFO : PROGRESS: at 89.22% examples, 461057 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:38:39,822 : INFO : PROGRESS: at 90.12% examples, 461973 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:38:40,839 : INFO : PROGRESS: at 90.81% examples, 461760 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-27 14:38:41,863 : INFO : PROGRESS: at 91.66% examples, 462367 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:38:42,869 : INFO : PROGRESS: at 92.29% examples, 461925 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:38:43,882 : INFO : PROGRESS: at 93.18% examples, 462764 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:38:44,912 : INFO : PROGRESS: at 93.83% examples, 462241 words/s, in_qsize 8, out_qsize 2\n",
      "2017-08-27 14:38:45,925 : INFO : PROGRESS: at 94.72% examples, 463063 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-27 14:38:46,936 : INFO : PROGRESS: at 95.37% examples, 462712 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:38:47,951 : INFO : PROGRESS: at 96.27% examples, 463411 words/s, in_qsize 7, out_qsize 1\n",
      "2017-08-27 14:38:48,992 : INFO : PROGRESS: at 97.01% examples, 463383 words/s, in_qsize 8, out_qsize 1\n",
      "2017-08-27 14:38:49,999 : INFO : PROGRESS: at 97.89% examples, 464041 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:38:51,013 : INFO : PROGRESS: at 98.56% examples, 463656 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:38:52,017 : INFO : PROGRESS: at 99.41% examples, 464232 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-27 14:38:52,998 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-08-27 14:38:53,003 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-08-27 14:38:53,006 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-08-27 14:38:53,007 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-08-27 14:38:53,008 : INFO : training on 88984735 raw words (63743607 effective words) took 137.5s, 463620 effective words/s\n",
      "2017-08-27 14:38:53,011 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-08-27 14:38:53,200 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
      "2017-08-27 14:38:53,201 : INFO : not storing attribute syn0norm\n",
      "2017-08-27 14:38:53,202 : INFO : not storing attribute cum_table\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-27 14:38:53,407 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print \"Training model...\"\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'berlin'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"france england germany berlin\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'london'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"paris berlin london austria\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'terrible', 0.8170218467712402),\n",
       " (u'horrible', 0.7606260776519775),\n",
       " (u'atrocious', 0.7489117383956909),\n",
       " (u'dreadful', 0.7323407530784607),\n",
       " (u'horrendous', 0.7301393151283264),\n",
       " (u'abysmal', 0.7289620637893677),\n",
       " (u'horrid', 0.6970181465148926),\n",
       " (u'lousy', 0.6664386987686157),\n",
       " (u'appalling', 0.6595410108566284),\n",
       " (u'laughable', 0.6260790824890137)]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"awful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-27 15:20:32,172 : INFO : loading Word2Vec object from 300features_40minwords_10context\n",
      "2017-08-27 15:20:32,287 : INFO : loading wv recursively from 300features_40minwords_10context.wv.* with mmap=None\n",
      "2017-08-27 15:20:32,288 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-08-27 15:20:32,289 : INFO : setting ignored attribute cum_table to None\n",
      "2017-08-27 15:20:32,290 : INFO : loaded 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec.load(\"300features_40minwords_10context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets clean the review\n",
    "\n",
    "def clean_reviews(review,remove_stopwords):\n",
    "    clean_1 = BeautifulSoup(review).get_text()\n",
    "    clean_2 = re.sub(\"[^a-zA-Z]\",\" \",clean_1)\n",
    "    clean_3 = clean_2.lower().split()\n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in clean_3 if not w in stop_words]\n",
    "    \n",
    "    return words\n",
    "    \n",
    "clean_train_reviews = []\n",
    "\n",
    "for review in train[\"review\"]:\n",
    "    clean_train_reviews.append( clean_reviews( review, remove_stopwords=True ))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.wv.syn0norm instead of  model.syn0norm\n",
    "# model.wv.syn0 instead of  model.syn0\n",
    "# model.wv.vocab instead of model.vocab\n",
    "# model.wv.index2word instead of  model.index2word\n",
    "import numpy as np\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    nwords = 0.\n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    for word in words:\n",
    "        # if the word is in wordset then add to feature vec\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    \n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "def get_average_feature_vectors( reviews, model, num_features):\n",
    "    counter = 0\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    \n",
    "    for review in reviews:\n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)\n",
    "        counter += 1\n",
    "        return reviewFeatureVecs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataVecs = get_average_feature_vectors( clean_train_reviews, model, num_features )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average feature vecs for test reviews\n"
     ]
    }
   ],
   "source": [
    "print \"Creating average feature vecs for test reviews\"\n",
    "clean_test_reviews = []\n",
    "for review in test[\"review\"]:\n",
    "    clean_test_reviews.append( clean_reviews( review, remove_stopwords=True ))\n",
    "\n",
    "testDataVecs = get_average_feature_vectors( clean_test_reviews, model, num_features )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a random forest to labeled training data...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier( n_estimators = 100 )\n",
    "\n",
    "print \"Fitting a random forest to labeled training data...\"\n",
    "forest = forest.fit( trainDataVecs, train[\"sentiment\"] )\n",
    "result = forest.predict( testDataVecs )\n",
    "\n",
    "# Write the test results \n",
    "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n",
    "output.to_csv( \"Word2Vec_AverageVectors.csv\", index=False, quoting=3 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for K Means clustering:  757.010732174 seconds.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "\n",
    "start = time.time() # Start time\n",
    "\n",
    "# Set \"k\" (num_clusters) to be 1/5th of the vocabulary size, or an\n",
    "# average of 5 words per cluster\n",
    "word_vectors = model.wv.syn0\n",
    "num_clusters = word_vectors.shape[0] / 5\n",
    "\n",
    "kmeans_clustering = KMeans( n_clusters = num_clusters )\n",
    "\n",
    "idx = kmeans_clustering.fit_predict( word_vectors )\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "elapsed = end - start\n",
    "print \"Time taken for K Means clustering: \", elapsed, \"seconds.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3297\n"
     ]
    }
   ],
   "source": [
    "word_centroid_map = dict(zip( model.wv.index2word, idx ))\n",
    "\n",
    "print len(set(word_centroid_map.values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 0\n",
      "[u'diversion', u'romp']\n",
      "\n",
      "Cluster 1\n",
      "[u'lesley', u'nailed', u'burns']\n",
      "\n",
      "Cluster 2\n",
      "[u'bolts', u'fire', u'carrying', u'fires', u'blasting', u'tilt', u'toting', u'shooting', u'firing']\n",
      "\n",
      "Cluster 3\n",
      "[u'deliberate', u'deliberately', u'purposely']\n",
      "\n",
      "Cluster 4\n",
      "[u'messiah', u'ruler', u'antichrist', u'protector', u'prophecy', u'emperor']\n",
      "\n",
      "Cluster 5\n",
      "[u'object', u'implication', u'orgasm', u'ego', u'symbol']\n",
      "\n",
      "Cluster 6\n",
      "[u'altering', u'examining', u'reflecting']\n",
      "\n",
      "Cluster 7\n",
      "[u'freed', u'governor', u'assassinated', u'president', u'senator', u'minister', u'probation']\n",
      "\n",
      "Cluster 8\n",
      "[u'bacon', u'costner', u'smith', u'kevin', u'spacey', u'kline']\n",
      "\n",
      "Cluster 9\n",
      "[u'trite', u'humorless', u'unremarkable', u'aimless', u'uninspiring', u'vapid', u'overwrought', u'stale', u'lifeless', u'insipid']\n"
     ]
    }
   ],
   "source": [
    "for cluster in xrange(0,10):\n",
    "    print \"\\nCluster %d\" % cluster\n",
    "    # Find all of the words for that cluster number, and print them out\n",
    "    words = []\n",
    "    for i in xrange(0,len(word_centroid_map.values())):\n",
    "        if( word_centroid_map.values()[i] == cluster ):\n",
    "            words.append(word_centroid_map.keys()[i])\n",
    "    print words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_bag_of_centroids( wordlist, word_centroid_map ):\n",
    "    #\n",
    "    # The number of clusters is equal to the highest cluster index\n",
    "    # in the word / centroid map\n",
    "    num_centroids = max( word_centroid_map.values() ) + 1\n",
    "    #\n",
    "    # Pre-allocate the bag of centroids vector (for speed)\n",
    "    bag_of_centroids = np.zeros( num_centroids, dtype=\"float32\" )\n",
    "    #\n",
    "    # Loop over the words in the review. If the word is in the vocabulary,\n",
    "    # find which cluster it belongs to, and increment that cluster count \n",
    "    # by one\n",
    "    for word in wordlist:\n",
    "        if word in word_centroid_map:\n",
    "            index = word_centroid_map[word]\n",
    "            bag_of_centroids[index] += 1\n",
    "    #\n",
    "    # Return the \"bag of centroids\"\n",
    "    return bag_of_centroids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_centroids = np.zeros( (train[\"review\"].size, num_clusters), dtype=\"float32\" )\n",
    "\n",
    "# Transform the training set reviews into bags of centroids\n",
    "counter = 0\n",
    "for review in clean_train_reviews:\n",
    "    train_centroids[counter] = create_bag_of_centroids( review, word_centroid_map )\n",
    "    counter += 1\n",
    "\n",
    "# Repeat for test reviews \n",
    "test_centroids = np.zeros(( test[\"review\"].size, num_clusters), dtype=\"float32\" )\n",
    "\n",
    "counter = 0\n",
    "for review in clean_test_reviews:\n",
    "    test_centroids[counter] = create_bag_of_centroids( review, word_centroid_map )\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a random forest to labeled training data...\n"
     ]
    }
   ],
   "source": [
    "# Fit a random forest and extract predictions \n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "# Fitting the forest may take a few minutes\n",
    "print \"Fitting a random forest to labeled training data...\"\n",
    "forest = forest.fit(train_centroids,train[\"sentiment\"])\n",
    "result = forest.predict(test_centroids)\n",
    "\n",
    "# Write the test results \n",
    "output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":result})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['sentiment'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"This movie is a disaster within a disaster film. It is full of great action scenes, which are only meaningful if you throw away all sense of reality. Let\\'s see, word to the wise, lava burns you; steam burns you. You can\\'t stand next to lava. Diverting a minor lava flow is difficult, let alone a significant one. Scares me to think that some might actually believe what they saw in this movie.<br /><br />Even worse is the significant amount of talent that went into making this film. I mean the acting is actually very good. The effects are above average. Hard to believe somebody read the scripts for this and allowed all this talent to be wasted. I guess my suggestion would be that if this movie is about to start on TV ... look away! It is like a train wreck: it is so awful that once you know what is coming, you just have to watch. Look away and spend your time on more meaningful content.\"'"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['review'][1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
